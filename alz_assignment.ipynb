{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda840dc568eb63462e89066e3ebf52f43f",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from adni.load_data import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting learning curves\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generates 1 plot: the test and training learning curve.\n",
    "    \"\"\"\n",
    "    axes.set_title(title)\n",
    "    # if ylim is not None:\n",
    "    #    axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, _, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=StratifiedKFold(n_splits=10), n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                      train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                      color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                      test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                      color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "              label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "              label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The number of samples: 855\nThe number of columns: 268\nThe number of AD samples: 519 (60.7%)\nThe number of CN samples: 336 (39.3%)\n"
    }
   ],
   "source": [
    "# Describe data\n",
    "AMOUNT_SAMPLES = len(DATA.index)\n",
    "AMOUNT_FEATURES = len(DATA.columns)\n",
    "print(f'The number of samples: {AMOUNT_SAMPLES}')\n",
    "print(f'The number of columns: {AMOUNT_FEATURES}')\n",
    "\n",
    "AMOUNT_AD = sum(DATA['label'] == 'AD')\n",
    "AMOUNT_CN = sum(DATA['label'] == 'CN')\n",
    "RATIO_AD = AMOUNT_AD/AMOUNT_SAMPLES\n",
    "RATIO_CN = AMOUNT_CN/AMOUNT_SAMPLES\n",
    "print(f'The number of AD samples: {AMOUNT_AD} ({round(RATIO_AD*100,2)}%)')\n",
    "print(f'The number of CN samples: {AMOUNT_CN} ({round(RATIO_CN*100,2)}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information\n",
    "# Get dataframe X with all features and dataframe Y with labels\n",
    "X = DATA\n",
    "X = X.drop(['label'], axis=1)\n",
    "Y = DATA['label']\n",
    "\n",
    "# Function for outer cross validation\n",
    "CV_10FOLD = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Lists for AUC scores\n",
    "AUC_KNN = []\n",
    "AUC_RF = []\n",
    "AUC_SVM = []\n",
    "\n",
    "# Lists for sensitivity scores\n",
    "SENS_KNN = []\n",
    "SENS_RF = []\n",
    "SENS_SVM = []\n",
    "\n",
    "# Lists for specificity scores\n",
    "SPEC_KNN = []\n",
    "SPEC_RF = []\n",
    "SPEC_SVM = []\n",
    "\n",
    "# lists for different hyperparameter sets\n",
    "PAR_KNN = []\n",
    "PAR_RF = []\n",
    "PAR_SVM = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer and inner crossvalidation\n",
    "for train_index, test_index in CV_10FOLD.split(X, Y):\n",
    "    # Split the data\n",
    "    X_TRAIN = X.iloc[train_index]\n",
    "    Y_TRAIN = Y.iloc[train_index]\n",
    "\n",
    "    X_TEST = X.iloc[test_index]\n",
    "    Y_TEST = Y.iloc[test_index]\n",
    "\n",
    "    # PREPROCESSING\n",
    "\n",
    "    # Remove duplicates in X and corresponding Y\n",
    "    DUPLICATES = X_TRAIN[X_TRAIN.duplicated(keep='first')]\n",
    "    DUPLICATES_ID = DUPLICATES.index\n",
    "    X_TRAIN = X_TRAIN.drop(DUPLICATES_ID)\n",
    "    Y_TRAIN = Y_TRAIN.drop(DUPLICATES_ID)\n",
    "\n",
    "    DUPLICATES_TEST = X_TEST[X_TEST.duplicated(keep='first')]\n",
    "    DUPLICATES_ID_TEST = DUPLICATES_TEST.index\n",
    "    X_TEST = X_TEST.drop(DUPLICATES_ID_TEST)\n",
    "    Y_TEST = Y_TEST.drop(DUPLICATES_ID_TEST)\n",
    "\n",
    "    # Binarize labels\n",
    "    LB = preprocessing.LabelBinarizer()\n",
    "    Y_TRAIN = LB.fit_transform(Y_TRAIN)\n",
    "    Y_TEST = LB.fit_transform(Y_TEST)\n",
    "\n",
    "    # Remove duplicate features\n",
    "    X_TRAIN = X_TRAIN.T.drop_duplicates().T\n",
    "    X_TEST = X_TEST.T.drop_duplicates().T\n",
    "\n",
    "    # Remove empty columns\n",
    "    EMPTY_COLS = X_TRAIN.columns[(X_TRAIN == 0).sum() > 0.8*X_TRAIN.shape[0]]\n",
    "    X_TRAIN = X_TRAIN.drop(X_TRAIN[EMPTY_COLS], axis=1)\n",
    "\n",
    "    EMPTY_COLS_TEST = X_TEST.columns[(X_TEST == 0).sum() > 0.8*X_TEST.shape[0]]\n",
    "    X_TEST = X_TEST.drop(X_TEST[EMPTY_COLS_TEST], axis=1)\n",
    "\n",
    "    # Removal of columns with same values\n",
    "    NUNIQUE = X_TRAIN.apply(pd.Series.nunique)\n",
    "    SAME_COLS = NUNIQUE[NUNIQUE < 2].index\n",
    "    X_TRAIN = X_TRAIN.drop(X_TRAIN[SAME_COLS], axis=1)\n",
    "\n",
    "    NUNIQUE_TEST = X_TEST.apply(pd.Series.nunique)\n",
    "    SAME_COLS_TEST = NUNIQUE_TEST[NUNIQUE < 2].index\n",
    "    X_TEST = X_TEST.drop(X_TEST[SAME_COLS_TEST], axis=1)\n",
    "\n",
    "    # Scaling: Robust range matching\n",
    "    SCALER = preprocessing.RobustScaler()\n",
    "    SCALER.fit(X_TRAIN)\n",
    "    X_TRAIN = SCALER.transform(X_TRAIN)\n",
    "    X_TEST = SCALER.transform(X_TEST)\n",
    "\n",
    "    # CLASSIFIERS\n",
    "    SCORE = {'accuracy': 'accuracy',\n",
    "             'roc_auc': 'roc_auc'}\n",
    "    REFIT = 'roc_auc'\n",
    "\n",
    "    # Function for outer cross validation\n",
    "    CV_5 = StratifiedShuffleSplit(n_splits=5, test_size=0.10, train_size=0.90)\n",
    "\n",
    "    # K-Nearest Neighbors (KNN)\n",
    "\n",
    "    # Search for best hyperparameters for the combination of PCA and Nearest Neighbors\n",
    "    PIPE_KNN = Pipeline([('pca', PCA()),\n",
    "                         ('knn', KNeighborsClassifier())])\n",
    "\n",
    "    # The set of hyperparameters to tune\n",
    "    PARAMETERS_KNN = {'pca__n_components': [1, 2, 3, 4, 5, 10, 20, 50, 100, 150, 200],\n",
    "                      'knn__n_neighbors': list(range(1, 99, 2)),\n",
    "                      'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "    CLF_KNN = RandomizedSearchCV(PIPE_KNN, cv=CV_5, n_jobs=-1, n_iter=100,\n",
    "                                 param_distributions=PARAMETERS_KNN,\n",
    "                                 scoring=SCORE, refit=REFIT)\n",
    "\n",
    "    # Train\n",
    "    CLF_KNN.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "    # DataFrame of the results with the different hyperparameters\n",
    "    DF_KNN = pd.DataFrame(CLF_KNN.cv_results_)\n",
    "\n",
    "    CLF_KNN_BEST = CLF_KNN.best_estimator_\n",
    "    PAR_KNN.append(CLF_KNN.best_params_)\n",
    "\n",
    "    # Random Forest (RF)\n",
    "    # Search for best hyperparameters for the combination of PCA and RandomForestClassifier\n",
    "    PIPE_RF = Pipeline([('pca', PCA()),\n",
    "                        ('rf', RandomForestClassifier())])\n",
    "\n",
    "    # The set of hyperparameters to tune\n",
    "    PARAMETERS_RF = {'pca__n_components': [1, 2, 3, 4, 5, 10, 20, 50, 100, 150, 200],\n",
    "                     'rf__n_estimators': list(range(10, 200, 10)),\n",
    "                     'rf__max_features': ['auto', 'sqrt'],\n",
    "                     'rf__max_depth': list(range(10, 50, 10)),\n",
    "                     'rf__min_samples_split': [2, 5, 10],\n",
    "                     'rf__min_samples_leaf': [1, 2, 4],\n",
    "                     'rf__bootstrap': [True, False]}\n",
    "\n",
    "    CLF_RF = RandomizedSearchCV(PIPE_RF, cv=CV_5, n_jobs=-1, n_iter=100,\n",
    "                                param_distributions=PARAMETERS_RF,\n",
    "                                scoring=SCORE, refit=REFIT)\n",
    "\n",
    "    # Train\n",
    "    CLF_RF.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "    # DataFrame of the results with the different hyperparameters\n",
    "    DF_RF = pd.DataFrame(CLF_RF.cv_results_)\n",
    "\n",
    "    CLF_RF_BEST = CLF_RF.best_estimator_\n",
    "    PAR_SVM.append(CLF_RF.best_params_)\n",
    "\n",
    "    # Support Vector Machine (SVM)\n",
    "\n",
    "    # Search for best hyperparameters for the combination of PCA and SVM\n",
    "    PIPE_SVM = Pipeline([('pca', PCA()),\n",
    "                         ('svc', SVC())])\n",
    "\n",
    "    # The set of hyperparameters to tune\n",
    "    PARAMETERS_SVM = [{'svc__kernel': ['rbf'], 'svc__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                       'svc__C': [0.01, 0.1, 0.5, 1, 10, 50, 100], 'svc__max_iter': [1000],\n",
    "                       'pca__n_components': [1, 2, 3, 4, 5, 10, 20, 50, 100, 150, 200]},\n",
    "                      {'svc__kernel': ['sigmoid'], 'svc__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                       'svc__C': [0.01, 0.1, 0.5, 1, 10, 50, 100], 'svc__max_iter': [1000], \n",
    "                       'svc__coef0' : list(np.arange(0,1,0.1)),\n",
    "                       'pca__n_components': [1, 2, 3, 4, 5, 10, 20, 50, 100, 150, 200]},\n",
    "                      {'svc__kernel': ['poly'], 'svc__degree': [2, 3, 4, 5],\n",
    "                       'svc__C': [0.01, 0.1, 0.5, 1, 10, 50, 100], 'svc__max_iter': [1000],\n",
    "                       'svc__coef0' : list(np.arange(0,1,0.1)),\n",
    "                       'pca__n_components': [1, 2, 3, 4, 5, 10, 20, 50, 100, 150, 200]}]\n",
    "\n",
    "    CLF_SVM = RandomizedSearchCV(PIPE_SVM, cv=CV_5, n_jobs=-1, n_iter=100,\n",
    "                                 param_distributions=PARAMETERS_SVM,\n",
    "                                 scoring=SCORE, refit=REFIT)\n",
    "\n",
    "    # Train\n",
    "    CLF_SVM.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "    # DataFrame of the results with the different hyperparameters\n",
    "    DF_SVM = pd.DataFrame(CLF_SVM.cv_results_)\n",
    "\n",
    "    CLF_SVM_BEST = CLF_SVM.best_estimator_\n",
    "    PAR_RF.append(CLF_RF.best_params_)\n",
    "\n",
    "    # Train with outer cross-validation\n",
    "    CLF_KNN_BEST.fit(X_TRAIN, Y_TRAIN)\n",
    "    CLF_RF_BEST.fit(X_TRAIN, Y_TRAIN)\n",
    "    CLF_SVM_BEST.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "    # get predictions\n",
    "    KNN_prediction = CLF_KNN_BEST.predict(X_TEST)\n",
    "    RF_prediction = CLF_RF_BEST.predict(X_TEST)\n",
    "    SVM_prediction = CLF_SVM_BEST.predict(X_TEST)\n",
    "\n",
    "    AUC_KNN.append(roc_auc_score(Y_TEST, KNN_prediction))\n",
    "    AUC_RF.append(roc_auc_score(Y_TEST, RF_prediction))\n",
    "    AUC_SVM.append(roc_auc_score(Y_TEST, SVM_prediction))\n",
    "\n",
    "    TN_KNN, FP_KNN, FN_KNN, TP_KNN = confusion_matrix(Y_TEST, KNN_prediction).ravel()\n",
    "    SPEC_KNN.append(TN_KNN / (TN_KNN+FP_KNN))\n",
    "    SENS_KNN.append(TP_KNN / (TP_KNN+FN_KNN))\n",
    "\n",
    "    TN_RF, FP_RF, FN_RF, TP_RF = confusion_matrix(Y_TEST, RF_prediction).ravel()\n",
    "    SPEC_RF.append(TN_RF / (TN_RF+FP_RF))\n",
    "    SENS_RF.append(TP_RF / (TP_RF+FN_RF))\n",
    "\n",
    "    TN_SVM, FP_SVM, FN_SVM, TP_SVM = confusion_matrix(Y_TEST, SVM_prediction).ravel()\n",
    "    SPEC_SVM.append(TN_SVM / (TN_SVM+FP_SVM))\n",
    "    SENS_SVM.append(TP_SVM / (TP_SVM+FN_SVM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "\n",
    "# PREPROCESSING X AND Y\n",
    "\n",
    "# Remove duplicates in X and corresponding Y\n",
    "DUPLICATES = X[X.duplicated(keep='first')]\n",
    "DUPLICATES_ID = DUPLICATES.index\n",
    "X = X.drop(DUPLICATES_ID)\n",
    "Y = Y.drop(DUPLICATES_ID)\n",
    "\n",
    "# Binarize labels\n",
    "LB = preprocessing.LabelBinarizer()\n",
    "Y = LB.fit_transform(Y)\n",
    "\n",
    "# Remove duplicate features\n",
    "X = X.T.drop_duplicates().T\n",
    "\n",
    "# Remove empty columns\n",
    "EMPTY_COLS = X.columns[(X == 0).sum() > 0.8*X.shape[0]]\n",
    "X = X.drop(X[EMPTY_COLS], axis=1)\n",
    "\n",
    "# Removal of columns with same values\n",
    "NUNIQUE = X.apply(pd.Series.nunique)\n",
    "SAME_COLS = NUNIQUE[NUNIQUE < 2].index\n",
    "X = X.drop(X[SAME_COLS], axis=1)\n",
    "\n",
    "# Scaling: Robust range matching\n",
    "SCALER = preprocessing.RobustScaler()\n",
    "SCALER.fit(X)\n",
    "X = SCALER.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "CLFS = [CLF_KNN_BEST, CLF_RF_BEST, CLF_SVM_BEST]\n",
    "TITLE_CLF = ['KNN', 'RF', 'SVM']\n",
    "\n",
    "fig, AXES = plt.subplots(1, 3, figsize=(20, 10))\n",
    "NUM = 0\n",
    "for CLF, TITLE_CLF in zip(CLFS, TITLE_CLF):\n",
    "    title = f'Learning Curve {TITLE_CLF}'\n",
    "    plot_learning_curve(CLF, title, X, Y, axes=AXES[NUM])\n",
    "    NUM += 1\n",
    "fig.savefig(f'learning_curves 3 classifiers.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results\n",
    "\n",
    "# Create df with AUC-values of the tree classifiers over the 10-fold cross validation\n",
    "DF_AUC_RESULTS = pd.DataFrame({'KNN': list(AUC_KNN),\n",
    "                               'RF': list(AUC_RF),\n",
    "                               'SVM': list(AUC_SVM)})\n",
    "DF_AUC_RESULTS.loc['mean'] = DF_AUC_RESULTS.mean()\n",
    "DF_AUC_RESULTS['best_clf'] = DF_AUC_RESULTS.idxmax(axis=1)\n",
    "DF_AUC_RESULTS.loc['std'] = DF_AUC_RESULTS.std()\n",
    "print(DF_AUC_RESULTS)\n",
    "DF_AUC_RESULTS.to_csv('df_AUC_results.csv')\n",
    "\n",
    "# Create df with specificities\n",
    "DF_SPEC_RESULTS = pd.DataFrame({'KNN': list(SPEC_KNN),\n",
    "                                'RF': list(SPEC_RF),\n",
    "                                'SVM': list(SPEC_SVM)})\n",
    "DF_SPEC_RESULTS.loc['mean'] = DF_SPEC_RESULTS.mean()\n",
    "DF_SPEC_RESULTS.loc['std'] = DF_SPEC_RESULTS.std()\n",
    "print(DF_SPEC_RESULTS)\n",
    "DF_SPEC_RESULTS.to_csv('df_SPEC_results.csv')\n",
    "\n",
    "# Create df with sensitivities\n",
    "DF_SENS_RESULTS = pd.DataFrame({'KNN': list(SENS_KNN),\n",
    "                                'RF': list(SENS_RF),\n",
    "                                'SVM': list(SENS_SVM)})\n",
    "DF_SENS_RESULTS.loc['mean'] = DF_SENS_RESULTS.mean()\n",
    "DF_SENS_RESULTS.loc['std'] = DF_SENS_RESULTS.std()\n",
    "print(DF_SENS_RESULTS)\n",
    "DF_SENS_RESULTS.to_csv('df_SENS_results.csv')"
   ]
  }
 ]
}